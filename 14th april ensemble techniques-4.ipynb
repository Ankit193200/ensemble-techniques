{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "897c2c75-d8fe-42cf-b186-3c7361d3c50c",
   "metadata": {},
   "source": [
    "To accomplish the tasks you've outlined, we'll use Python with popular libraries such as pandas, scikit-learn, and matplotlib. Make sure to install these libraries if you haven't already:\n",
    "\n",
    "```bash\n",
    "pip install pandas scikit-learn matplotlib\n",
    "```\n",
    "\n",
    "Now, let's go through each step:\n",
    "\n",
    "### Q1: Preprocess the dataset\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://drive.google.com/uc?id=1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Handle missing values (if any)\n",
    "# For simplicity, let's drop rows with missing values in this example\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode categorical variables (if any)\n",
    "# Assuming there are no categorical variables in this dataset\n",
    "\n",
    "# Scale numerical features if necessary (e.g., using Min-Max scaling or Standardization)\n",
    "# You can use scikit-learn's StandardScaler for this\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[['age', 'resting_blood_pressure', 'serum_cholesterol', 'max_heart_rate_achieved']] = scaler.fit_transform(\n",
    "    df[['age', 'resting_blood_pressure', 'serum_cholesterol', 'max_heart_rate_achieved']]\n",
    ")\n",
    "\n",
    "# Display the preprocessed data\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "### Q2: Split the dataset\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split the dataset into a training set (70%) and a test set (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "```\n",
    "\n",
    "### Q3: Train a random forest classifier\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train a random forest classifier on the training set\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### Q4: Evaluate the performance\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "```\n",
    "\n",
    "### Q5: Feature Importance\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    {'Feature': X.columns, 'Importance': feature_importances}\n",
    ")\n",
    "\n",
    "# Display the top 5 most important features\n",
    "top_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(5)\n",
    "print(top_features)\n",
    "\n",
    "# Visualize feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_features['Feature'], top_features['Importance'])\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Top 5 Most Important Features')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Q6: Hyperparameter Tuning\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create the grid search\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,\n",
    "                           scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best set of hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the performance of the tuned model on the test set\n",
    "tuned_rf_classifier = grid_search.best_estimator_\n",
    "tuned_y_pred = tuned_rf_classifier.predict(X_test)\n",
    "\n",
    "tuned_accuracy = accuracy_score(y_test, tuned_y_pred)\n",
    "tuned_precision = precision_score(y_test, tuned_y_pred)\n",
    "tuned_recall = recall_score(y_test, tuned_y_pred)\n",
    "tuned_f1 = f1_score(y_test, tuned_y_pred)\n",
    "\n",
    "print(\"Tuned Model Performance:\")\n",
    "print(\"Accuracy:\", tuned_accuracy)\n",
    "print(\"Precision:\", tuned_precision)\n",
    "print(\"Recall:\", tuned_recall)\n",
    "print(\"F1 Score:\", tuned_f1)\n",
    "```\n",
    "\n",
    "### Q8: Interpret the Model\n",
    "\n",
    "To interpret the model, you can use dimensionality reduction techniques or plot decision boundaries. Since the dataset has many features, you can use techniques like PCA for dimensionality reduction and then plot the decision boundaries. Here's a simplified example:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "# Use PCA for dimensionality reduction (considering the top 2 features)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Train the model on the reduced dataset\n",
    "rf_classifier.fit(X_pca, y)\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='Set1', s=50)\n",
    "plt.title('Random Forest Decision Boundaries (Top 2 Features)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "\n",
    "# Plot decision boundaries\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 100), np.linspace(ylim[0], ylim[1], 100))\n",
    "Z = rf_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap='Set1')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Q7: Report and Compare\n",
    "\n",
    "After hyperparameter tuning, you can compare the performance of the tuned model with the default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7864c8-b61a-4934-b937-73789bf4b010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
